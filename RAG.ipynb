{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c07df793362ab09f",
   "metadata": {},
   "source": [
    "# Retrival Augmented generation(RAG) for DATA MINING Text Book"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed8907a753f89d3",
   "metadata": {},
   "source": [
    "# Data Loading and Preprocessing"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Import Libraries",
   "id": "ff9b4f4f26e59315"
  },
  {
   "cell_type": "code",
   "id": "6e0ffa3b2fa99505",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T22:12:26.578732Z",
     "start_time": "2025-10-23T22:12:21.597787Z"
    }
   },
   "source": [
    "# import necessary libraries\n",
    "import pymupdf\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bdcalling123\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\bdcalling123\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data Acquisition",
   "id": "bf34af19e33ada49"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T22:12:32.553328Z",
     "start_time": "2025-10-23T22:12:32.542014Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# load PDF files from the specified directory\n",
    "pdf_path = 'Reference Book.pdf' # data mining textbook\n",
    "doc = pymupdf.open(pdf_path)"
   ],
   "id": "74c565f1dcc26c65",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T22:12:34.983352Z",
     "start_time": "2025-10-23T22:12:34.927303Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "pdf_path = \"Reference Book.pdf\"\n",
    "with fitz.open(pdf_path) as doc:\n",
    "    print(f\"Num of Pages: {doc.page_count}\")\n",
    "    print(f\"Metadata: {doc.metadata}\")\n",
    "    print(f\"Table of Contents: {doc.get_toc()}\")\n",
    "\n",
    "    image_count = 0\n",
    "    link_count = 0\n",
    "    fonts = set()\n",
    "\n",
    "    for page_num in range(doc.page_count):\n",
    "        page = doc.load_page(page_num)\n",
    "        image_count += len(page.get_images(full=True))\n",
    "        link_count += len(page.get_links())\n",
    "        # No official page.get_fonts() — so this part will error!\n",
    "\n",
    "    embedded = doc.embedded_file_names() if hasattr(doc, \"embedded_file_names\") else []\n",
    "\n",
    "    print(f\"number of images on the document: {image_count}\")\n",
    "    print(f\"number of links on the document: {link_count}\")\n",
    "    print(f\"number of embedded files on the document: {len(embedded)}\")\n"
   ],
   "id": "cbe990b8f28c6124",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of Pages: 740\n",
      "Metadata: {'format': 'PDF 1.6', 'title': 'Data Mining. Concepts and Techniques, 3rd Edition (The Morgan Kaufmann Series in Data Management Systems)', 'author': 'Jiawei Han, Micheline Kamber, Jian Pei', 'subject': 'Morgan Kaufmann 2011', 'keywords': '0123814790\\r\\n9780123814791', 'creator': '', 'producer': '', 'creationDate': \"D:20151005145245+03'00'\", 'modDate': \"D:20151220163746+03'30'\", 'trapped': '', 'encryption': None}\n",
      "Table of Contents: [[1, 'Front Cover ', 1], [1, 'Data Mining: Concepts and Techniques', 6], [1, 'Copyright', 7], [1, 'Dedication', 8], [1, 'Table of Contents', 10], [1, 'Foreword', 20], [1, 'Foreword to Second Edition', 22], [1, 'Preface', 24], [1, 'Acknowledgments', 32], [1, 'About the Authors', 36], [1, 'Chapter 1. Introduction', 38], [2, '1.1 Why Data Mining?', 38], [2, '1.2 What Is Data Mining?', 42], [2, '1.3 What Kinds of Data Can Be Mined?', 45], [2, '1.4 What Kinds of Patterns Can Be Mined?', 52], [2, '1.5 Which Technologies Are Used?', 60], [2, '1.6 Which Kinds of Applications Are Targeted?', 64], [2, '1.7 Major Issues in Data Mining', 66], [2, '1.8 Summary', 70], [2, '1.9 Exercises', 71], [2, '1.10 Bibliographic Notes', 72], [1, 'Chapter 2. Getting to Know Your Data', 76], [2, '2.1 Data Objects and Attribute Types', 77], [2, '2.2 Basic Statistical Descriptions of Data', 81], [2, '2.3 Data Visualization', 93], [2, '2.4 Measuring Data Similarity and Dissimilarity', 102], [2, '2.5 Summary', 116], [2, '2.6 Exercises', 116], [2, '2.7 Bibliographic Notes', 118], [1, 'Chapter 3. Data Preprocessing', 120], [2, '3.1 Data Preprocessing: An Overview', 121], [2, '3.2 Data Cleaning', 125], [2, '3.3 Data Integration', 130], [2, '3.4 Data Reduction', 136], [2, '3.5 Data Transformation and Data Discretization', 148], [2, '3.6 Summary', 157], [2, '3.7 Exercises', 158], [2, '3.8 Bibliographic Notes', 160], [1, 'Chapter 4. Data Warehousing and Online Analytical Processing', 162], [2, '4.1 Data Warehouse: Basic Concepts', 162], [2, '4.2 Data Warehouse Modeling: Data Cube and OLAP', 172], [2, '4.3 Data Warehouse Design and Usage', 187], [2, '4.4 Data Warehouse Implementation', 193], [2, '4.5 Data Generalization by Attribute-Oriented Induction', 203], [2, '4.6 Summary', 215], [2, '4.7 Exercises', 217], [2, '4.8 Bibliographic Notes', 221], [1, 'Chapter 5. Data Cube Technology', 224], [2, '5.1 Data Cube Computation: Preliminary Concepts', 225], [2, '5.2 Data Cube Computation Methods', 231], [2, '5.3 Processing Advanced Kinds of Queries by Exploring Cube Technology', 255], [2, '5.4 Multidimensional Data Analysis in Cube Space', 264], [2, '5.5 Summary', 271], [2, '5.6 Exercises', 272], [2, '5.7 Bibliographic Notes', 277], [1, 'Chapter 6. Mining Frequent Patterns, Associations, and Correlations: Basic Concepts and Methods', 280], [2, '6.1 Basic Concepts', 280], [2, '6.2 Frequent Itemset Mining Methods', 285], [2, '6.3 Which Patterns Are Interesting?—Pattern Evaluation Methods', 301], [2, '6.4 Summary', 308], [2, '6.5 Exercises', 310], [2, '6.6 Bibliographic Notes', 313], [1, 'Chapter 7. Advanced Pattern Mining', 316], [2, '7.1 Pattern Mining: A Road Map', 316], [2, '7.2 Pattern Mining in Multilevel, Multidimensional Space', 320], [2, '7.3 Constraint-Based Frequent Pattern Mining', 331], [2, '7.4 Mining High-Dimensional Data and Colossal Patterns', 338], [2, '7.5 Mining Compressed or Approximate Patterns', 344], [2, '7.6 Pattern Exploration and Application', 350], [2, '7.7 Summary', 356], [2, '7.8 Exercises', 358], [2, '7.9 Bibliographic Notes', 360], [1, 'Chapter 8. Classification: Basic Concepts', 364], [2, '8.1 Basic Concepts', 364], [2, '8.2 Decision Tree Induction', 367], [2, '8.3 Bayes Classification Methods', 387], [2, '8.4 Rule-Based Classification', 392], [2, '8.5 Model Evaluation and Selection', 401], [2, '8.6 Techniques to Improve Classification Accuracy', 414], [2, '8.7 Summary', 422], [2, '8.8 Exercises', 423], [2, '8.9 Bibliographic Notes', 426], [1, 'Chapter 9. Classification: Advanced Methods', 430], [2, '9.1 Bayesian Belief Networks', 430], [2, '9.2 Classification by Backpropagation', 435], [2, '9.3 Support Vector Machines', 445], [2, '9.4 Classification Using Frequent Patterns', 452], [2, '9.5 Lazy Learners (or Learning from Your Neighbors)', 459], [2, '9.6 Other Classification Methods', 463], [2, '9.7 Additional Topics Regarding Classification', 466], [2, '9.8 Summary', 473], [2, '9.9 Exercises', 475], [2, '9.10 Bibliographic Notes', 476], [1, 'Chapter 10. Cluster Analysis: Basic Concepts and Methods', 480], [2, '10.1 Cluster Analysis', 481], [2, '10.2 Partitioning Methods', 488], [2, '10.3 Hierarchical Methods', 494], [2, '10.4 Density-Based Methods', 508], [2, '10.5 Grid-Based Methods', 516], [2, '10.6 Evaluation of Clustering', 520], [2, '10.7 Summary', 527], [2, '10.8 Exercises', 528], [2, '10.9 Bibliographic Notes', 531], [1, 'Chapter 11. Advanced Cluster Analysis', 534], [2, '11.1 Probabilistic Model-Based Clustering', 534], [2, '11.2 Clustering High-Dimensional Data', 545], [2, '11.3 Clustering Graph and Network Data', 559], [2, '11.4 Clustering with Constraints', 569], [2, '11.5 Summary', 575], [2, '11.6 Exercises', 576], [2, '11.7 Bibliographic Notes', 577], [1, 'Chapter 12. Outlier Detection', 580], [2, '12.1 Outliers and Outlier Analysis', 581], [2, '12.2 Outlier Detection Methods', 586], [2, '12.3 Statistical Approaches', 590], [2, '12.4 Proximity-Based Approaches', 597], [2, '12.5 Clustering-Based Approaches', 604], [2, '12.6 Classification-Based Approaches', 608], [2, '12.7 Mining Contextual and Collective Outliers', 610], [2, '12.8 Outlier Detection in High-Dimensional Data', 613], [2, '12.9 Summary', 618], [2, '12.10 Exercises', 619], [2, '12.11 Bibliographic Notes', 620], [1, 'Chapter 13. Data Mining Trends and Research Frontiers', 622], [2, '13.1 Mining Complex Data Types', 622], [2, '13.2 Other Methodologies of Data Mining', 635], [2, '13.3 Data Mining Applications', 644], [2, '13.4 Data Mining and Society', 655], [2, '13.5 Data Mining Trends', 659], [2, '13.6 Summary', 662], [2, '13.7 Exercises', 663], [2, '13.8 Bibliographic Notes', 665], [1, 'Bibliography', 670], [1, 'Index', 710]]\n",
      "number of images on the document: 25\n",
      "number of links on the document: 0\n",
      "number of embedded files on the document: 0\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Parsing and Cleaning",
   "id": "6292e5bc37f987de"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T22:12:39.857589Z",
     "start_time": "2025-10-23T22:12:39.854399Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import fitz\n",
    "import unicodedata\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.tokenize import sent_tokenize"
   ],
   "id": "f6b902b438dafdd5",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T22:12:44.381795Z",
     "start_time": "2025-10-23T22:12:42.141789Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Text and formula normalization helpers\n",
    "def normalize_unicode(text):\n",
    "    return unicodedata.normalize(\"NFKC\", text)\n",
    "\n",
    "def fix_hyphenation(text):\n",
    "    text = re.sub(r'-\\s*\\n\\s*', '', text)\n",
    "    text = re.sub(r'(?<!\\n)\\n(?!\\n)', ' ', text)\n",
    "    return text\n",
    "\n",
    "def strip_refs_and_pnums(text):\n",
    "    text = re.sub(r'\\[\\s*\\d+\\s*\\]', '', text)\n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    text = normalize_unicode(text)\n",
    "    text = fix_hyphenation(text)\n",
    "    text = strip_refs_and_pnums(text)\n",
    "    text = re.sub(r'[\\x00-\\x1f\\x7f]+', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Heading/Section detection (font size and regex)\n",
    "def is_heading(text, max_span, median_size, heading_scale):\n",
    "    heading_like = median_size > 0 and max_span >= median_size * heading_scale\n",
    "    regex_like = bool(re.match(r'^\\s*((Chapter|CHAPTER|Section|SECTION)[\\s\\d.:]+|(\\d+\\.)+\\s+\\w+)', text))\n",
    "    return heading_like or regex_like\n",
    "\n",
    "def is_tabular(text):\n",
    "    tabular_keywords = ['table', 'Tab', 'tabular']\n",
    "    return (text.count('\\t') > 2 or text.count('|') > 2 or len(re.findall(r'\\s{4,}', text)) > 1\n",
    "            or any(k.lower() in text.lower() for k in tabular_keywords))\n",
    "\n",
    "# Formula helpers\n",
    "_THEOREM_HEADING_RE = re.compile(r'^\\s*(Theorem|Lemma|Proposition|Corollary|Definition|Claim|Remark|Proof)\\b', re.IGNORECASE)\n",
    "_LATEX_INLINE_RE = re.compile(r'(\\$\\$.*?\\$\\$|\\$.*?\\$|\\\\\\[.*?\\\\\\]|\\\\\\(.+?\\\\\\))', re.DOTALL)\n",
    "_MATH_SYMBOLS = set(list(\"αβγδεζηθικλμνξοπρστυφχψωΑΒΓΔΕΖΗΘΙΚΛΜΝΞΟΠΡΣΤΥΦΧΨΩ∑∏∫√∞≤≥±≈⇒⇔∀∃∈∉∂×÷→←|=^_<>\"))\n",
    "\n",
    "def detect_formulas_in_text(text, symbol_threshold=3, frac_threshold=0.02):\n",
    "    formulas = []\n",
    "    for m in _LATEX_INLINE_RE.finditer(text):\n",
    "        formulas.append(m.group(0).strip())\n",
    "    candidates = re.split(r'(?<=[\\.\\;\\:\\n])\\s+', text)\n",
    "    for seg in candidates:\n",
    "        seg = seg.strip()\n",
    "        if not seg: continue\n",
    "        sym_count = sum(1 for ch in seg if ch in _MATH_SYMBOLS)\n",
    "        if sym_count >= symbol_threshold or (sym_count / max(1, len(seg)) > frac_threshold):\n",
    "            if seg not in formulas:\n",
    "                formulas.append(seg)\n",
    "    return formulas\n",
    "\n",
    "def _save_image_from_xref(doc, page, xref, out_dir, prefix=\"formula\"):\n",
    "    try: pix = fitz.Pixmap(doc, xref)\n",
    "    except Exception: return None\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    fn = f\"{prefix}_p{page.number+1}_xref{xref}.png\"\n",
    "    out_path = os.path.join(out_dir, fn)\n",
    "    try:\n",
    "        if pix.n < 5: pix.save(out_path)\n",
    "        else:\n",
    "            pix0 = fitz.Pixmap(fitz.csRGB, pix)\n",
    "            pix0.save(out_path)\n",
    "            pix0 = None\n",
    "        pix = None\n",
    "    except Exception: return None\n",
    "    return out_path\n",
    "\n",
    "def _image_bbox_if_available(page, xref):\n",
    "    if hasattr(page, \"get_image_bbox\"):\n",
    "        try: return page.get_image_bbox(xref)\n",
    "        except Exception: return None\n",
    "    return None\n",
    "\n",
    "def extract_pdf_structure(pdf_path, min_block_len=20, heading_scale=1.25, out_image_dir=\"formulas\"):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    top_cands, bot_cands = [], []\n",
    "    page_blocks = []\n",
    "\n",
    "    for pno in range(doc.page_count):\n",
    "        page = doc.load_page(pno)\n",
    "        d = page.get_text(\"dict\")\n",
    "        blocks = [] ; sizes = []\n",
    "        for b in d[\"blocks\"]:\n",
    "            if b.get(\"type\", 0) == 0:\n",
    "                for line in b.get(\"lines\", []):\n",
    "                    for span in line.get(\"spans\", []):\n",
    "                        sizes.append(span.get(\"size\", 0))\n",
    "        median_size = (sorted(sizes)[len(sizes)//2] if sizes else 0)\n",
    "        image_xrefs = []\n",
    "        for img in page.get_images(full=True):\n",
    "            xref = img[0]\n",
    "            ibox = _image_bbox_if_available(page, xref)\n",
    "            image_xrefs.append({\"xref\": xref, \"bbox\": ibox})\n",
    "\n",
    "        for b in d[\"blocks\"]:\n",
    "            if b.get(\"type\", 0) != 0: continue\n",
    "            text = \" \".join(span.get(\"text\",\"\") for line in b.get(\"lines\", []) for span in line.get(\"spans\", []))\n",
    "            text = text.strip()\n",
    "            if not text or len(text) < min_block_len: continue\n",
    "            max_span = max(span.get(\"size\", 0) for line in b.get(\"lines\", []) for span in line.get(\"spans\", [])) if b.get(\"lines\") else 0\n",
    "            fonts = [span.get(\"font\",\"\") for line in b.get(\"lines\", []) for span in line.get(\"spans\", [])]\n",
    "            heading_candidate = is_heading(text, max_span, median_size, heading_scale)\n",
    "            tabular_candidate = is_tabular(text)\n",
    "            theorem_like = _THEOREM_HEADING_RE.match(text)\n",
    "            formulas_in_block = detect_formulas_in_text(text)\n",
    "            blocks.append({\n",
    "                \"text\": text,\n",
    "                \"bbox\": b.get(\"bbox\"),\n",
    "                \"is_heading\": heading_candidate,\n",
    "                \"is_tabular\": tabular_candidate,\n",
    "                \"is_formula\": bool(formulas_in_block),  # Not strict: just means math detected\n",
    "                \"formulas\": formulas_in_block,\n",
    "                \"fonts\": fonts,\n",
    "                \"theorem_like\": bool(theorem_like),\n",
    "                \"max_span\": max_span\n",
    "            })\n",
    "\n",
    "        sorted_blocks = sorted(blocks, key=lambda x: x[\"bbox\"][1])\n",
    "        if sorted_blocks:\n",
    "            for b in sorted_blocks[:2]:\n",
    "                if 0 < len(b[\"text\"]) < 120: top_cands.append(b[\"text\"])\n",
    "            for b in sorted_blocks[-2:]:\n",
    "                if 0 < len(b[\"text\"]) < 120: bot_cands.append(b[\"text\"])\n",
    "        page_blocks.append({\"pno\": pno + 1, \"blocks\": blocks, \"image_xrefs\": image_xrefs})\n",
    "\n",
    "    header = Counter(top_cands).most_common(1)\n",
    "    footer = Counter(bot_cands).most_common(1)\n",
    "    header_text = header[0][0] if header and header[0][1] > max(2, len(doc)//10) else None\n",
    "    footer_text = footer[0][0] if footer and footer[0][1] > max(2, len(doc)//10) else None\n",
    "\n",
    "    # Chunk by headings, attach tables/images/formulas, merge/split for optimal size\n",
    "    chunks = []\n",
    "    for page in page_blocks:\n",
    "        current_heading = None\n",
    "        accum = []\n",
    "        bbox_accum = []\n",
    "        tabular_blocks = []\n",
    "        formulas_in_chunk = []\n",
    "        formula_images_chunk = []\n",
    "        for b in page[\"blocks\"]:\n",
    "            if header_text and b[\"text\"] == header_text: continue\n",
    "            if footer_text and b[\"text\"] == footer_text: continue\n",
    "            if b[\"is_heading\"]:\n",
    "                if accum:\n",
    "                    raw = \" \".join(accum)\n",
    "                    cleaned = clean_text(raw)\n",
    "                    if cleaned:\n",
    "                        # Find formula images by proximity (like your previous proximity logic)\n",
    "                        chunk_images = []\n",
    "                        if bbox_accum:\n",
    "                            xs = [ (bb[0]+bb[2])/2 for bb in bbox_accum ]\n",
    "                            ys = [ (bb[1]+bb[3])/2 for bb in bbox_accum ]\n",
    "                            cx = sum(xs)/len(xs); cy = sum(ys)/len(ys)\n",
    "                            for xrefinfo in page[\"image_xrefs\"]:\n",
    "                                ib = xrefinfo.get(\"bbox\")\n",
    "                                if ib:\n",
    "                                    icx = (ib[0]+ib[2])/2\n",
    "                                    icy = (ib[1]+ib[3])/2\n",
    "                                    dist = ((icx-cx)**2 + (icy-cy)**2)**0.5\n",
    "                                    if dist<=150 or (icx>=min(xs)-10 and icx<=max(xs)+10):\n",
    "                                        saved = _save_image_from_xref(doc, doc.load_page(page[\"pno\"]-1), xrefinfo[\"xref\"], out_image_dir, prefix=\"formula\")\n",
    "                                        if saved: chunk_images.append(saved)\n",
    "                        chunks.append({\n",
    "                            \"source\": pdf_path,\n",
    "                            \"pages\": (page[\"pno\"], page[\"pno\"]),\n",
    "                            \"heading\": current_heading,\n",
    "                            \"text\": cleaned,\n",
    "                            \"bbox\": bbox_accum,\n",
    "                            \"images\": chunk_images,  # Only formula images near the chunk\n",
    "                            \"tables\": tabular_blocks,\n",
    "                            \"formulas\": formulas_in_chunk,\n",
    "                            \"confidence\": 0.9\n",
    "                        })\n",
    "                    accum, bbox_accum, tabular_blocks, formulas_in_chunk, formula_images_chunk = [],[],[],[],[]\n",
    "                current_heading = b[\"text\"]\n",
    "                continue\n",
    "            # Main chunk population\n",
    "            accum.append(b[\"text\"])\n",
    "            bbox_accum.append(b[\"bbox\"])\n",
    "            if b[\"is_tabular\"]: tabular_blocks.append(b[\"text\"])\n",
    "            if b[\"is_formula\"]: formulas_in_chunk.extend(b[\"formulas\"])\n",
    "        # flush page end\n",
    "        if accum:\n",
    "            raw = \" \".join(accum)\n",
    "            cleaned = clean_text(raw)\n",
    "            if cleaned:\n",
    "                # Formula image proximity\n",
    "                chunk_images = []\n",
    "                if bbox_accum:\n",
    "                    xs = [ (bb[0]+bb[2])/2 for bb in bbox_accum ]\n",
    "                    ys = [ (bb[1]+bb[3])/2 for bb in bbox_accum ]\n",
    "                    cx = sum(xs)/len(xs); cy = sum(ys)/len(ys)\n",
    "                    for xrefinfo in page[\"image_xrefs\"]:\n",
    "                        ib = xrefinfo.get(\"bbox\")\n",
    "                        if ib:\n",
    "                            icx = (ib[0]+ib[2])/2\n",
    "                            icy = (ib[1]+ib[3])/2\n",
    "                            dist = ((icx-cx)**2 + (icy-cy)**2)**0.5\n",
    "                            if dist<=150 or (icx>=min(xs)-10 and icx<=max(xs)+10):\n",
    "                                saved = _save_image_from_xref(doc, doc.load_page(page[\"pno\"]-1), xrefinfo[\"xref\"], out_image_dir, prefix=\"formula\")\n",
    "                                if saved: chunk_images.append(saved)\n",
    "                chunks.append({\n",
    "                    \"source\": pdf_path,\n",
    "                    \"pages\": (page[\"pno\"], page[\"pno\"]),\n",
    "                    \"heading\": current_heading,\n",
    "                    \"text\": cleaned,\n",
    "                    \"bbox\": bbox_accum,\n",
    "                    \"images\": chunk_images,\n",
    "                    \"tables\": tabular_blocks,\n",
    "                    \"formulas\": formulas_in_chunk,\n",
    "                    \"confidence\": 0.85\n",
    "                })\n",
    "\n",
    "    # Postprocess: merge small, split large, preserve formulas/images\n",
    "    merged = []\n",
    "    i = 0\n",
    "    while i < len(chunks):\n",
    "        cur = chunks[i]\n",
    "        text = cur[\"text\"]\n",
    "        if len(text) < 300 and i+1 < len(chunks) and chunks[i+1][\"source\"] == cur[\"source\"]:\n",
    "            nxt = chunks[i+1]\n",
    "            merged.append({\n",
    "                \"source\": cur[\"source\"],\n",
    "                \"pages\": (cur[\"pages\"][0], nxt[\"pages\"][1]),\n",
    "                \"heading\": cur[\"heading\"] or nxt[\"heading\"],\n",
    "                \"text\": clean_text(cur[\"text\"] + \" \" + nxt[\"text\"]),\n",
    "                \"bbox\": cur[\"bbox\"] + nxt[\"bbox\"],\n",
    "                \"images\": cur[\"images\"] + nxt[\"images\"],\n",
    "                \"tables\": cur.get(\"tables\",[]) + nxt.get(\"tables\",[]),\n",
    "                \"formulas\": cur.get(\"formulas\",[]) + nxt.get(\"formulas\",[]),\n",
    "                \"confidence\": max(cur[\"confidence\"], nxt[\"confidence\"])\n",
    "            })\n",
    "            i += 2\n",
    "        else:\n",
    "            if len(text) > 1200:\n",
    "                sents = sent_tokenize(text)\n",
    "                cur_block = \"\"\n",
    "                for s in sents:\n",
    "                    if len(cur_block) + len(s) + 1 <= 800:\n",
    "                        cur_block = (cur_block + \" \" + s).strip()\n",
    "                    else:\n",
    "                        merged.append({**cur, \"text\": cur_block})\n",
    "                        cur_block = s\n",
    "                if cur_block:\n",
    "                    merged.append({**cur, \"text\": cur_block})\n",
    "            else:\n",
    "                merged.append(cur)\n",
    "            i += 1\n",
    "\n",
    "    return merged\n",
    "chunks = extract_pdf_structure(pdf_path)"
   ],
   "id": "bf22bd6e6e08a6d4",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Chunk Embedding and Indexing",
   "id": "d4a27ad9e92db3f5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T22:20:43.872288Z",
     "start_time": "2025-10-23T22:20:43.844800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(chunks)\n",
    "print(df.head())"
   ],
   "id": "31cddc88c9aff4af",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               source   pages heading  \\\n",
      "0  Reference Book.pdf  (3, 3)    None   \n",
      "1  Reference Book.pdf  (3, 3)    None   \n",
      "2  Reference Book.pdf  (4, 4)    None   \n",
      "3  Reference Book.pdf  (4, 4)    None   \n",
      "4  Reference Book.pdf  (4, 4)    None   \n",
      "\n",
      "                                                text  \\\n",
      "0                                                      \n",
      "1  The Morgan Kaufmann Series in Data Management ...   \n",
      "2                                                      \n",
      "3  Data Modeling Essentials, 3 rd Edition Graeme ...   \n",
      "4  Hall Joe Celko’s Data and Databases: Concepts ...   \n",
      "\n",
      "                                                bbox images  \\\n",
      "0  [(118.4000015258789, 81.22136688232422, 401.36...     []   \n",
      "1  [(118.4000015258789, 81.22136688232422, 401.36...     []   \n",
      "2  [(148.39999389648438, 81.15653228759766, 487.1...     []   \n",
      "3  [(148.39999389648438, 81.15653228759766, 487.1...     []   \n",
      "4  [(148.39999389648438, 81.15653228759766, 487.1...     []   \n",
      "\n",
      "                                              tables formulas  confidence  \n",
      "0  [Joe Celko’s Data, Measurements, and Standards...       []        0.85  \n",
      "1  [Joe Celko’s Data, Measurements, and Standards...       []        0.85  \n",
      "2  [Data Modeling Essentials,  3 rd Edition Graem...       []        0.85  \n",
      "3  [Data Modeling Essentials,  3 rd Edition Graem...       []        0.85  \n",
      "4  [Data Modeling Essentials,  3 rd Edition Graem...       []        0.85  \n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T22:23:44.234410Z",
     "start_time": "2025-10-23T22:22:40.131542Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#embedding chunks using sentence transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\")  # or domain-specific model\n",
    "\n",
    "# Embed the text of each chunk\n",
    "df['embedding'] = df['text'].apply(lambda x: model.encode(x))\n"
   ],
   "id": "b14bf96facf676ad",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bdcalling123\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\bdcalling123\\.cache\\huggingface\\hub\\models--sentence-transformers--paraphrase-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T22:27:18.623672Z",
     "start_time": "2025-10-23T22:26:44.277975Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def chunk_context(row):\n",
    "    context = row['text']\n",
    "    if row['formulas']:\n",
    "        context += \" \" + \" \".join(row['formulas'])\n",
    "    if row['tables']:\n",
    "        context += \" \" + \" \".join(row['tables'])\n",
    "    return context\n",
    "\n",
    "df['embedding'] = df.apply(chunk_context, axis=1).apply(lambda x: model.encode(x))\n"
   ],
   "id": "8e7b8fed7dcbcf1d",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# index the embeddings",
   "id": "1ae038c09cbc2ed"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T22:29:08.365926Z",
     "start_time": "2025-10-23T22:29:08.336576Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "embeddings = np.stack(df['embedding'].to_list())\n",
    "dim = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dim)\n",
    "index.add(embeddings)\n",
    "\n",
    "# Save the index for later\n",
    "faiss.write_index(index, \"ds_book_faiss.index\")"
   ],
   "id": "64e7201d239ce823",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T22:44:18.357960Z",
     "start_time": "2025-10-23T22:44:18.339782Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = \"What is the data mining?\"\n",
    "query_emb = model.encode(query)\n",
    "\n",
    "D, I = index.search(np.array([query_emb]), k=5)  # Get top-5 matches\n",
    "for idx in I[0]:\n",
    "    print(df.iloc[idx][['heading', 'pages', 'text']])\n"
   ],
   "id": "91108ea91c3a6290",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heading                                                 None\n",
      "pages                                             (191, 191)\n",
      "text       Yet according to this view, data mining covers...\n",
      "Name: 518, dtype: object\n",
      "heading         13.4.1  Ubiquitous and Invisible Data Mining\n",
      "pages                                             (655, 655)\n",
      "text       Data mining is present in many aspects of our ...\n",
      "Name: 2050, dtype: object\n",
      "heading    Steps 1 through 4 are different forms of data ...\n",
      "pages                                               (45, 45)\n",
      "text       8 Chapter 1 Introduction As a general technolo...\n",
      "Name: 70, dtype: object\n",
      "heading    Another challenge for both content-based and c...\n",
      "pages                                             (655, 655)\n",
      "text       618 Chapter 13 Data Mining Trends and Research...\n",
      "Name: 2049, dtype: object\n",
      "heading                            1.7.1  Mining Methodology\n",
      "pages                                               (66, 66)\n",
      "text       Mining various and new kinds of knowledge: Dat...\n",
      "Name: 133, dtype: object\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2dc88096fab81538"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
